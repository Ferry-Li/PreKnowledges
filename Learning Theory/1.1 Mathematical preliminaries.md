# 1 Linear Algebra and Differentiable Calculus

### 1.1 Minimization of quadratic forms

Given a positive definite matrix $A\in \R^{n\times n}$ and a vector $b \in \R^n$, then minimization of quadratic forms with linear terms can be done in closed form as:
$$
\inf_{x\in\R^n}\frac{1}{2}x^TAx-b^Tx=-\frac{1}{2}b^TA^{-1}b,
$$
with minimizer $x_*=A^{-1}b$. If $A$ is in fact not invertible and $b$ is not in the column space of $A$, then the infimum is $-\infin$.

Note that this result is often used in various forms, such as
$$
b^Tx \leqslant \frac{1}{2}b^TA^{-1}b + \frac{1}{2}x^TAx
$$
with equality if and only if $b=Ax$.

### 1.2 Inverting a $2\times 2$ matrix

Let $M=\begin{pmatrix} a & b \\ c & d\end{pmatrix}$. If $ad-bc \ne 0$, then we may invert it as follows
$$
M^{-1}=\frac{1}{ad-bc}\begin{pmatrix}
d & -b \\
-c & a
\end{pmatrix}.
$$
This can be generalized to matrices defined by blocks.

### 1.3 Inverting matrices defined by blocks & Matrix inversion lemma

![image-20221005212628150](C:\Users\21156\AppData\Roaming\Typora\typora-user-images\image-20221005212628150.png)

This shows that: $(M/A)=D-CA^{-1}B$
$$
M^{-1}=\begin{pmatrix}
A & B \\
C & D
\end{pmatrix}^{-1}=\begin{pmatrix}
A^{-1}+A^{-1}B(M/A)^{-1}CA^{-1} & -A^{-1}B(M/A)^{-1} \\
-(M/A)^{-1}CA^{-1} & (M/A)^{-1}
\end{pmatrix}.
$$
By moving zero to the upper-right block, and assuming $D$ and $(M/D)=A-BD^{-1}C$, we obtain:
$$
M^{-1}=\begin{pmatrix}
A & B \\
C & D
\end{pmatrix}^{-1}=\begin{pmatrix}
(M/D)^{-1}  & -(M/D)^{-1}CD^{-1} \\
-D^{-1}B(M/D)^{-1} & D^{-1}+D^{-1}C(M/D)^{-1}BD^{-1}
\end{pmatrix}.
$$
With two equations above, we obtain the identities:
$$
(A-BD^{-1}C)^{-1}=A^{-1}+A^{-1}B(D-CA^{-1}B)^{-1}CA^{-1}  \\
(D-CA^{-1}B)^{-1}=D^{-1}+D^{-1}C(A-BD^{-1}C)^{-1}BD^{-1}
$$
The lemma is often applied when $C=B^T,A=I$ and $D=-I$， which leads to
$$
(I+BB^T)^{-1}=I-B(I+B^TB)^{-1}B^T,
$$
multiply $B$ on both sides, and we obtain a compact formula:
$$
(I+BB^T)^{-1}B=B(I+B^TB)^{-1}.
$$

##### Block matrices $M$ and $M^{-1}$ can be diagonalized as:

$$
M=\begin{pmatrix}
A & B \\
C & D
\end{pmatrix}=
\begin{pmatrix}
I & 0 \\
CA^{-1} & I
\end{pmatrix}
\begin{pmatrix}
A & 0 \\
0 & (M/A)
\end{pmatrix}
\begin{pmatrix}
I & A^{-1}B \\
0 & I
\end{pmatrix} \\
M^{-1}=\begin{pmatrix}
A & B \\
C & D
\end{pmatrix}^{-1}=
\begin{pmatrix}
I & -A^{-1}B \\
0 & I
\end{pmatrix}
\begin{pmatrix}
A^{-1} & 0 \\
0 & (M/A)^{-1}
\end{pmatrix}
\begin{pmatrix}
I & 0 \\
-CA^{-1} & I
\end{pmatrix}
$$

##### Conditional covariances matrices for Gaussian vectors

$\mu_{y|x}=\mu_y+\Sigma_{yx}\Sigma_{xx}^{-1}(x-\mu_x)$

$\Sigma_{y|x}=\Sigma_{yy}-\Sigma_{yx}\Sigma_{xx}^{-1}\Sigma_{xy}$

### 1.4 Differential calculus

Some cases in matrix notations:

**Quadratic forms: ** $f(x)=\frac{1}{2}x^TAx-b^Tx,f'(x)=Ax-b,f''(x=I)$

**Least-squares: ** $f(x)=\frac{1}{2}||y-Xw||_2^2,f'(x)=X^T(Xw-y),f''(x)=X^TX$.



# 2 Concentration inequalities

All results rely on the simple probabilistic assumption that data are independently and identically distributed (i.i.d). 

**A key insight: ** when you have $n$ independent zero-mean random variables, the natural "magnitude" of their average is $1/\sqrt n$ times smaller than their average magnitude.

###### Example

If $Z_1,...,Z_n\in\R$ are independent and identically distributed with $\sigma^2=\mathbb E(Z-\mathbb EZ)^2$, then 
$$
var(\frac{1}{n}\sum_{i=1}^nZ_i)=\frac{1}{n^2}\sum_{i=1}^nvar(Z_i)=\frac{\sigma^2}{n}.
$$
The equality can be interpreted as
$$
\mathbb E(\frac{1}{n}\sum_{i=1}^nZ_i-\mathbb EZ)^2=\frac{\sigma^2}{n}.
$$
**Union Bound: ** $\mathbb P(\bigcup_{f\in\mathcal{F}}A_f) \leqslant \sum_{f\in\mathcal{F}} \mathbb P(A_f)$.

It has a direct application in upper-bounding the tail probability of the supremum of random variables:
$$
\mathbb P(\sup_{f\in\mathcal F}Z_f>t)=\mathbb P(\bigcup_{f\in\mathcal F}\{Z_f>t\}) \leqslant \sum_{f\in\mathcal{F}}\mathbb P(Z_f>t).
$$

### 2.1 Hoeffding’s inequality

For any $\epsilon>0,t>0$, $X_i\in[A_i, B_i]$:
$$
\mathbb{P}(\sum_{i=1}^n(X_i-\mathbb{E}X_i)\geq \epsilon)\leq \exp(-t\epsilon+\frac{t^2}{8} \sum_{i=1}^n(B_i-A_i)^2)
$$

###### Proof.

To simplify, set $Y_i=X_i-\mathbb{E}[X_i]$. Thus, $\mathbb{E}[Y_i]=0$, and $Y_i\in [A_i-\mathbb{E}[X_i], B_i-\mathbb{E}[X_i]]=[a_i,b_i]$.

To make sure the term be positive, we apply exp operation to switch $\mathbb{P}(\sum Y_i\ge\sigma)$ into $\mathbb{P}(e^{t\sum Y_i}\ge e^{t\epsilon})$.

With Markov inequality ($\mathbb{P}(|X|\ge a) \le \frac{\mathbb{E}X}{a}$) and i.i.d, we have $\mathbb{P}(e^{t\sum Y_i} \ge e^{t\epsilon})\ge e^{-t\epsilon}\mathbb{E}[e^{t\sum Y_i}]=e^{-t\epsilon}\prod\mathbb{E}[e^{tY_i}].$ 

Set $Y_i=(1-\alpha)a_i+\alpha b_i,\alpha \in [0, 1]$, and $\alpha = \frac{Y_i-a_i}{b_i-a_i},1-\alpha=\frac{b_i-Y_i}{b_i-a_i}$.

So, $e^{tY_i}=e^{(1-\alpha)ta_i+\alpha tb_i} \le (1-\alpha)e^{ta_i}+\alpha e^{tb_i}= \frac{b_i-Y_i}{b_i-a_i}e^{ta_i}+\frac{Y_i-a_i}{b_i-a_i}e^{tb_i}\le\frac{b_i}{b_i-a_i}e^{ta_i}-\frac{a_i}{b_i-a_i}e^{tb_i}$.

Set $\gamma=-\frac{a_i}{b_i-a_i},1-\gamma=\frac{b_i}{b_i-a_i},u=t(b_i-a_i)$.

Then $\frac{b_i}{b_i-a_i}e^{ta_i}-\frac{a_i}{b_i-a_i}e^{tb_i}=(1-\gamma)e^{-ru}+\gamma e^{u(1-\gamma)}=e^{-\gamma u}(1-\gamma+\gamma e^u)=(*)$.

Set $g(u)=\log (*)=-ru+\log(1-\gamma+\gamma e^u)$.

For $g(u)$, we have $g(0)=0,g'(u)=-r+\frac{re^u}{1-r+re^u},g'(0)=0,g''(u)=\frac{re^u(1-r+re^u)-re^ure^u}{(...)^2}\le\frac{re^u(1-r)}{(2\sqrt{re^u(1-r)})^2}=\frac{1}{4}$.

With Taylor, $g(u)\le g(0)+ug'(0)+\frac{u^2}{2}g''(u)=\frac{u^2}{8}=\frac{t^2}{8}(b_i-a_i)^2$. So we have $e^{tY_i} \le \frac{t^2}{8}(b_i-a_i)^2$, and with backward, we obtain $e^{-t\epsilon}\prod\mathbb{E}[e^{tY_i}] \le e^{-t\epsilon}\prod\frac{t^2}{8}(b_i-a_i)^2=\exp(-t\epsilon+\frac{t^2}{8}\sum(B_i-A_i)^2)$.



With $t=\frac{\epsilon}{\frac{1}{4}\sum(B_i-A_i)^2}$, the right term will be $\exp(-\frac{2\epsilon^2}{\sum(B_i-A_i)^2})$.

Its form can be shifted by multiply $1/n$: $\mathbb{P}(\frac{1}{n}\sum Z_i-\frac{1}{n}\mathbb{E}[Z_i]\ge t)\le\exp(-2nt^2)$. ($\epsilon=nt$)