# Part1: Highlights of Linear Algebra

Five basic problems:
$$
Ax=b, \; Ax=\lambda x, \; Av=\sigma u, \; \min \frac{||Ax||^2}{||x||^2}, \; \bf{Factor\;the\;matrix\;A}
$$

- Is the vector $b$ in the column space of $A$?
- eigenvector directions -> $Ax$ keeps the same direction as $x$.
- The SVD finds its simplest pieces $\sigma uv^T$. Data science meets linear algebra in the SVD.
- Least squares and principal components.

## 1.1 Multiplication $Ax$ Using Columns of $A$

Matrix-vector multiplication, column space and the rank.

### Matrix-vector Multiplication

$$
\begin{bmatrix}
2 & 3 \\
2 & 4 \\
3 & 7 \\
\end{bmatrix}
\begin{bmatrix}
x_1 \\ x_2
\end{bmatrix}
=x_1\begin{bmatrix}
2 \\ 2 \\ 3
\end{bmatrix}+
x_2\begin{bmatrix}
3 \\ 4 \\ 7
\end{bmatrix}=
combinations\;of\;columns\;\alpha_1\;and\;\alpha_2
$$

$Ax$ is a **linear combination** of the columns of $A$.

**The combinations of the columns** fill out the **column space** of $A$.

$b=(b_1, b_2, b_3)$ is in the column space of $A$ exactly when $Ax=b$ has a solution $(x_1, x_2)$. The solution $x$ shows how to express the right side $b$ as a combination $x_1a_1+x_2a_2$ of the columns. For some $b$ this is impossible —— they are not in the column space.

#### Some subspaces of $R^3$

- The **zero vector** $(0, 0, 0)$ by itself.

- A **line** of all vectors $x_1a_1$.
- A **plane** of all vectors $x_1a_1+x_2a_2$.
- The **whole $R^3$** with all vectors $x_1a_1+x_2a_2+x_3a_3$.

And the vectors $a_1, a_2, a_3$ should be **independent**.

### Independent Columns and the Rank of $A$

A **basis** for a subspace is a full set of independent vectors: **All vectors in the space are combinations of the basis vectors.**

**The rank of a matrix  is the dimension of its column space.**

**The number of independent columns equals the number of independent rows.**

## 1.2 Matrix-Matrix Multiplication $AB$

Another way to compute $AB=C$: columns of $A$ times rows of $B$.

![image-20220917155512549](C:\Users\21156\AppData\Roaming\Typora\typora-user-images\image-20220917155512549.png)

All columns of $uv^T$ are multiples of $u=[2,2,1]^T$. All rows are multiples of $v^T=[3,4,6]$.

**Row rank = Column rank.**

**$r$ independent columns $\Leftrightarrow$ $r$ independent rows**.

![image-20220917160752388](C:\Users\21156\AppData\Roaming\Typora\typora-user-images\image-20220917160752388.png)

**Outer product**: $AB$ is the sum of columns $a_k$ times rows $b^*_k$, which are **rank one matrices**.

A dominant theme in applied linear algebra is closely related to the outer product: **Factor $A$ into $CR$ and look at the pieces $c_kr_k^*$ of $A=CR$**.

#### Five important factorizations

- $A=LU$ comes from **elimination**. The matrix $L$ is lower triangular and $U$ is upper triangular.
- $A=QR$ comes from **orthogonalizing** the columns $a_1$ to $a_n$ as in "Gram-Schmidt". $Q$ has orthonormal columns ($Q^TQ=I$) and $R$ is upper triangular.
- $S=Q\Lambda Q^T$ comes from the **eigenvalues** of a symmetric matrix $S=S^T$. Eigenvalues on the diagonal of $\Lambda$. Orthonormal eigenvectors in the columns of $Q$.
- $A=X\Lambda X^{-1}$ is **diagonalization** when $A$ is $n$ by $n$ with $n$ independent eigenvectors.
- $A=U\Sigma V^T$ is the **Singular Value Decomposition** of any matrix $A$ (square or not).

## 1.3 The Four Fundamental Subspaces

The **column space** $C(A)$ contains all combinations of the columns of $A$.

The **row space** $C(A)$ contains all combinations of the columns of $A^T$.

The **nullspace** $N(A)$ contains all solutions $x$ to $Ax=0$.

The **left nullspace** $N(A^T)$ contains all solutions $y$ to $A^Ty=0$.

**Counting Law:** $r$ independent equations $Ax=0$ have $n-r$ independent solutions.

#### From a graph

![image-20220918103437314](C:\Users\21156\AppData\Roaming\Typora\typora-user-images\image-20220918103437314.png)

![image-20220918103453011](C:\Users\21156\AppData\Roaming\Typora\typora-user-images\image-20220918103453011.png)

The **nullspace $N(A)$**: set $b=0$. The special solution $x=(1, 1, 1, 1)$ is a basis for $N(A)$. The dimension of $N(A)$ is 1.

The **column space $C(A)$**: There must be $n-r=4-1=3$ independent columns.

The **row space $C(A^T)$**: The same as for columns.

The **left nullspace $N(A^T)$**: The dimension $m-r=5-3=2$.

![image-20220918105331440](C:\Users\21156\AppData\Roaming\Typora\typora-user-images\image-20220918105331440.png)

![image-20220918105511754](C:\Users\21156\AppData\Roaming\Typora\typora-user-images\image-20220918105511754.png)

#### Ranks of $AB$ and $A+B$

##### inequalities and equalities for the rank

- $r(AB) \le R(A), r(AB) \le r(B)$
- $r(A+B)\le r(A)+r(B)$
- $r(A^TA)=r(AA^T)=r(A)=r(A^T)$
- $r(A^{m\times r})=r(B^{r\times n})=r \Rightarrow r(AB)=r$

## 1.4 Elimination and $A=LU$

Elimination factored $A=LU$ into a lower triangular $L$ times an upper triangular $U$. $Ax=b$ split into $Lc=b$ and $Ux=c$. 

Row-Column: $a_{ij}:=(a_{ij}-\sum a_{ik}a_{kj})/a_{ii}$.

Pivots can't be zero. Good codes will choose the largest number to be the pivot.

Every invertible $n$ by $n$ matrix $A$ leads to $PA=LU,P=$ **permutation**.

#### An example of LU factorization

$$
\begin{bmatrix}
1
\end{bmatrix}
$$



## 1.5 Orthogonal Matrices and Subspaces

#### 1. Orthogonal vectors

$||x-y||^2=||x||^2+||y||^2-2||x||\;||y||\cos\theta$.

Orthogonal vectors have $\cos\theta=0$, and the last term disappears.

#### 2. Orthogonal basis

**Hadamard matrices**, if divided by the length of columns, are orthogonal bases in 2, 4, 8, 16, ... dimensions.

**Fact**: Every subspace of $R^n$ has an orthogonal basis. For two independent vector $a$ and $b$, if $c=b-\frac{a^Tb}{a^Ta}a$, then $a^Tc=a^Tb-a^Tb=0$. (Gram-Schmidt idea: a basis becomes an orthogonal basis).

#### 3. Orthogonal subspaces

The row space of $A$ is orthogonal to the nullspace of $A$.

The column space of $A$ is orthogonal to the nullspace of $A^T$.
$$
Ax=\begin{bmatrix}
row\;1 \\
\dots \\
row\;m
\end{bmatrix}
\begin{bmatrix}
x
\end{bmatrix}=\begin{bmatrix}
0 \\
\dots \\
0
\end{bmatrix}
\qquad
A^Ty=\begin{bmatrix}
(column\;1)^T \\
\dots \\
(column\;n)^T
\end{bmatrix}
\begin{bmatrix}
y
\end{bmatrix}=\begin{bmatrix}
0 \\
\dots \\
0
\end{bmatrix}
$$
![image-20220918171103853](C:\Users\21156\AppData\Roaming\Typora\typora-user-images\image-20220918171103853.png)

Singular vectors $Av_r=\sigma_r u_r$ finds orthonormal bases $v_1, \dots, v_r$ (left part in the picture) for the row space of $A$, and $u_1, \dots, u_r$ (right part of the picture) for the column space of $A$.

#### 4. Tall thin $Q$ with orthonormal columns: $Q^TQ=I$

All the matrices $P=QQ^T$ have $P^2=P$: $P^2=(QQ^T)(QQ^T)=Q(Q^TQ)Q^T=QQ^T=P$.

![image-20220918174326713](C:\Users\21156\AppData\Roaming\Typora\typora-user-images\image-20220918174326713.png)

![image-20220918174348713](C:\Users\21156\AppData\Roaming\Typora\typora-user-images\image-20220918174348713.png)

#### 5 Orthogonal matrices

For 2 by 2, they are **rotations** of the plane or **reflections**.
$$
Q_{rotate}=\begin{bmatrix}
\cos\theta & -\sin\theta \\
\sin\theta & \cos\theta
\end{bmatrix} \\
Q_{reflect}=\begin{bmatrix}
\cos\theta & \sin\theta \\
\sin\theta & -\cos\theta
\end{bmatrix}
$$
$Q_{rotate}$ is rotation through an angle $\theta$. $Q_{reflect}$ is reflection across the $\frac{\theta}{2}$ - line.

$Q_1Q_2$ is still orthogonal: $(Q_1Q_2)^T(Q_1Q_2)=Q_2^TQ_1^TQ_1Q_2=Q_2^TQ_2=I$.

rotation times rotation = rotation. reflection times reflection = rotation.

rotation times reflection = reflection.

#### Householder Reflections

$H_n=I-2uu^T=I-\frac{2}{n}ones(n,n)$.

$H^TH=H^2=(I-2uu^T)(I-2uu^T)=I-4uu^T+4uu^Tuu^T=I$.

$Hu=(I-2uu^T)u=u-2u=-u$.

$Hw=(I-2uu^T)w=w,(u^Tw=0)$ .

#### Hadamard matrices

![image-20221002162718761](C:\Users\21156\AppData\Roaming\Typora\typora-user-images\image-20221002162718761.png)

## 1.6 Eigenvalues and eigenvectors

$Ax=\lambda x, A^2x=\lambda^2x,...,A^{-1}x=\frac{1}{\lambda}x$.

The eigenvectors of $A$ don't change direction when you multiply when them by $A$.  The output $Ax$ is on the same line as the input vector $x$.

**Properties**

- Trace of $S$: $\sum \lambda=trace(S)$.
- Determinant: $\Pi \lambda=\det(S)$.
- Real eigenvalues: Symmetric matrices always have real eigenvalues.
- Orthogonal eigenvectors: $\lambda_i \ne \lambda_j \Rightarrow x_i \cdot x_j=0$.
- The eigenvectors of a ream matrix $A$ are orthogonal if and only if $A^TA=AA^T$.

#### Computing the Eigenvalues

$Ax=\lambda x \Rightarrow (A-\lambda I)x=0 \Rightarrow \det(A-\lambda I)=0$ 

The eigenvalues are real when $A$ is symmetric.

##### Notice : Shift in $A$ → shift in every $\lambda$

$(A+sI)x=\lambda x+sx=(\lambda+s)x$.

#### Similar Matrices

For every invertible matrix $B$, the eigenvalues of $BAB^{-1}$ are the same as the eigenvalues of $A$: $Ax=\lambda x \Rightarrow (BAB^{-1})(Bx)=BAx=B\lambda x=\lambda(Bx)$.

The matrices $BAB^{-1}$ (for every invertible $B$) are "similar" to $A$ : same eigenvalues.

With this idea, some large complex matrices $A-\lambda I$ can be converted to $BAB^{-1}$ gradually into a triangular matrix.

The **eigenvalues** of any **triangular matrix** $\begin{bmatrix} a & b\\ 0 & d \end{bmatrix}$ are $\lambda_1=a$ and $\lambda_2=d$.

#### Diagonalizing a Matrix

**Invertible** X is composed of eigenvectors $x_1, ..., x_n$.
$$
A\begin{bmatrix}
 \\
x_1 & .. & x_n \\
 \\
\end{bmatrix}
=
\begin{bmatrix}
 \\
Ax_1 & .. & Ax_n \\
 \\
\end{bmatrix}
=
\begin{bmatrix}
 \\
\lambda_1x_1 & .. &\lambda_n x_n \\
 \\
\end{bmatrix}
=
\begin{bmatrix}
 \\
x_1 & .. & x_n \\
 \\
\end{bmatrix}
\begin{bmatrix}
 \lambda_1 & & \\
 & \ddots &  \\
 &  & \lambda_n \\
\end{bmatrix}
$$
$AX=X\Lambda \Rightarrow A=X\Lambda X^{-1}$.

If we know the eigenvalues and eigenvectors, we know the matrix $A, A^2, ...$.

$A^2=(X\Lambda X^{-1})(X\Lambda X^{-1})=X\Lambda^2X^{-1}$: The eigenvectors of $A^k$ are $\lambda_1^k, ..., \lambda_n^k$, and the eigenvectors of $A^k$ are the same as the eigenvectors of $A$.

#### Non-diagonalizable Matrices

**Geometric Multiplicity=GM**: Count the **independent eigenvectors** for $\lambda$. Look at the dimension of the nullspace of $A-\lambda I$.

**Algebraic Multiplicity=AM**: Count the repetitions of $\lambda$ among the eigenvalues. Look at the root of $\det(A-\lambda I)=0$.

The shortage of eigenvectors when **GM < AM** means that $A$ is not diagonalizable. There is no invertible eigenvector matrix, thus formula $A=X\Lambda X^{-1}$ fails.

## 1.7 Symmetric Positive Definite Matrices

**Properties**

- All $n$ eigenvalues $\lambda$ of a symmetric matrix $S$ are real numbers.
- The $n$ eigenvectors $q$ can be chosen orthogonal, -> rescale to be orthonormal.

**Spectral Theorem**

Every real symmetric matrix has the form $S=Q\Lambda Q^T$.

For complex matrices, when $\bar{S}^T=S$, all eigenvalues of $S$ are real.

#### Positive Definite Matrices

**1.** Eigenvalues of a positive definite matrix are all positive.

#### The Energy-based Definition

**2.** $S$ is positive definite if the energy $x^TSx$ is positive for all vectors $x \ne 0$.

If $Sx=\lambda x $, then $x^TSx=\lambda x^Tx$. So $\lambda>0$ leads to $x^TSx>0$.

If $x^TSx>0$ for the eigenvectors of $S$, then $x^TSx>0$ for every nonzero vector $x$:
$$
\begin{aligned}
x^TSx &=(c_1x_1^T+\cdots+c_nx_n^T)S(c_1x_1+\cdots+c_nx_n) \\
&=(c_1x_1^T+\cdots+c_nx_n^T)(c_1\lambda_1x_1+\cdots+c_n\lambda_nx_n) \\
&=c_1^2\lambda_1x_1^Tx_1+\cdots+c_n^2\lambda_nx_n^Tx_n>0
\end{aligned}
$$
if every $\lambda_i>0$.

If $S_1$ and $S_2$ are symmetric positive definite, so is $S_1+S_2$.

**3.** $S=A^TA$ for a matrix $A$ with independent columns.

$Energy=x^TSx=x^TA^TAx=(Ax)^T(Ax)=||Ax||^2$. To assume $||Ax||^2 > 0$ , (namely $Ax\ne0$) when $x\ne0$, the columns of $A$ must be independent.

**Two special choices: **

1. If $S=Q\Lambda Q^T$, take square roots of those eigenvalues. Then $A=Q\sqrt{\Lambda }Q^T=A^T$.
2. If $S=LU=LDL^T$ with positive pivots in $D$, then $S=(L\sqrt{D})(\sqrt{D}L^T)$.

**4. ** All the leading determinants $D_1,D_2,...,D_n$ of $S$ are positive.

**5. ** All the pivots of $S$ are positive (in elimination). And the $k$ th pivot equals to the ratio $\frac{D_k}{D_{k-1}}$ of the leading determinants.

*****. Elimination factors every positive definite $S$ into $A^TA$  with $\sqrt{pivot}$ on the main diagonal ($A$ is upper triangular).

## 1.8 Singular Values and Singular Vectors in the SVD

#### Form

**1. ** For singular vectors, each $Av$ equals to $\sigma u$:
$$
Av_1=\sigma_1u_1 \; ... \; Av_r=\sigma_ru_r \; , \; \; Av_{r+1}=0 \; ... \; Av_n=0
$$

$r$ is the rank of $A$, $v_i$'s are orthogonal in $\R^n$, and $u_i$'s are orthogonal in $\R^m$. We will have $r$ positive singular values in descending order $\sigma_1\ge\sigma_2\ge...\ge\sigma_r>0$. The last $n-r$ $v$'s are in the nullspace of $A$, and the last $m-r$ $u$'s are in the nullspace of $A^T$.

**2. ** Write **1.** in matrix form.

![image-20220927110951051](C:\Users\21156\AppData\Roaming\Typora\typora-user-images\image-20220927110951051.png)

As $V$ and $U$ are orthogonal matrices (columns of $U$ and $V$ are orthogonal), $V^T=V^{-1}$ and $U^T=U^{-1}$. 

**3. ** The usual and famous expression of SVD: $A=U\Sigma V^T$.

Column-row multiplication of $U\Sigma$ times $V^T$ separates $A$ into $r$ **pieces** of rank 1: $A=U\Sigma V^T=\sigma_1u_1v_1^T+...+\sigma_ru_rv_r^T$.

#### Reduced form of SVD

Remove the parts that sure to produce zero.

![image-20220927160408101](C:\Users\21156\AppData\Roaming\Typora\typora-user-images\image-20220927160408101.png)

#### Importance of SVD

Like other factorizations, SVD separates the matrix into rank one pieces. A special property of the SVD is that **those pieces come in order of importance**. The sum of the first $k$ pieces is best possible for rank $k$:

$A_k=\sigma_1u_1v_1^T+...+\sigma_ku_kv_k^T$ is the best rank $k$ approximation to matrix $A$.



**Eckart-Young: **If $B$ has rank $k$, then $||A-A_k||\le||A-B||$.

#### First Proof of the SVD

Goal: $A=U\Sigma V^T$
$$
A^TA=(V\Sigma ^TU^T)(U\Sigma V^T)=V\Sigma^T\Sigma V^T \\
AA^T=(U\Sigma V^T)(V\Sigma^TU^T)=U\Sigma\Sigma^TU^T
$$
$V$ contains orthogonal eigenvalues of $A^TA$. $U$ contains orthogonal eigenvalues of $AA^T$.

$\sigma_1^2$ to $\sigma_r^2$ are the nonzero eigenvalues of both $A^TA$ and $AA^T$.

Starts with $v$ and $u_k=\frac{Av_k}{\sigma_k}$.

#### Process of SVD

1. Compute the eigenvalues $\lambda_1, ..., \lambda_r, \lambda_{r+1}...=\lambda_n=0$ and corresponding orthonormal eigenvectors $v_1, ... ,v_r, v_{r+1}, ..., v_n$.
2. $V_{n\times n}=(v_1, v_2, ..., v_r, v_{r+1}, ..., v_n)_{n\times n}$.
3. Compute orthonormal vectors $u_i=\frac{1}{\sqrt{\lambda_i}}Av_i$.
4. Expand $u_1, ... , u_r$ into $\R^m$'s orthonormal  bases $u_1, ..., u_r, b_1, ..., b_{m-r}$. $U_{m\times m}=(u_1, u_2, ..., u_r, b_1, ...,b_{m-r})_{m\times m}$.
5. $A_{m\times n}=U_{m\times m}\Sigma_{m \times n}V^T_{n \times n}$.

#### Some questions

1. If $S=Q\Lambda Q^T$ is symmetric positive definite, then $U\Sigma V^T=Q\Lambda Q^T$. Singular values are equal to eigenvalues.
2. If $S=Q\Lambda Q^T$ has a negative eigenvalue ($Sx=-\alpha x$), then the singular value will be $\sigma=+\alpha>0$, and one singular vector (either $u$ or $v$) must be $-x$. Then $Sx=-\alpha x$ is still the same as $Sv=\sigma u$.
3. If $A=Q$ is an orthogonal matrix, then eigenvalues are ±1, and singular values are equal to 1.
4. All eigenvalues of a square matrix $A$ are less than or equal to $\sigma_1$, because $||Ax||=||U\Sigma V^Tx||=||\Sigma V^Tx|| \le \sigma_1||V^Tx||=\sigma_1||x||$. With $||Ax||=|\lambda|\;||x||\le\sigma_1||x||$, we have $|\lambda|\le\sigma_1$.

#### Geometry of the SVD

![image-20221002144300116](C:\Users\21156\AppData\Roaming\Typora\typora-user-images\image-20221002144300116.png)

A 2 by 2 invertible matrix example. The four numbers $a, b, c, d$ in the matrix connect to two angles $\theta$ and $\phi$ and two numbers $\sigma_1$ and $\sigma_2$.
$$
\begin{bmatrix}
a & b \\
c & d
\end{bmatrix}=
\begin{bmatrix}
\cos\theta & -\sin\theta \\
\sin\theta & \cos\theta
\end{bmatrix}
\begin{bmatrix}
\sigma_1 &  \\
 & \sigma_2
\end{bmatrix}
\begin{bmatrix}
\cos\phi & \sin\phi \\
-\sin\phi & \cos\phi
\end{bmatrix}
$$

#### The first singular vector $v_1$

Maximize the ratio $\frac{||Ax||}{||x||}$. The maximum is $\sigma_1$ at the vector $v=v_1$.
$$
\frac{||Ax||^2}{||x||^2}=\frac{x^TA^TAx}{x^Tx}=\frac{x^TSx}{x^Tx} \\
\frac{\partial}{\partial x_i}(x^Tx)=\frac{\partial}{\partial x_i}(x_1^2+...+x_n^2)=2(x)_i \\
\frac{\partial}{\partial x_i}(x^TSx)=2(Sx)_i 
$$
To set $\partial / \partial x_i (x^TSx/x^Tx)=0$, we have:
$$
(x^Tx)2(Sx_i)-(x^TSx)2(x_i)=0
$$
Thus, the maximum value of $\frac{x^TSx}{x^Tx}=\frac{||Ax||^2}{||x||^2}$ is an eigenvalue $\lambda$ 0f $S$. The eigenvector that maximizes is $x=v_1$, and the eigenvalue is $\lambda_1=\sigma_1^2$.

#### $AB$ and $BA$: Equal nonzero Eigenvalues

If $A$ is $m \times n$ and $B$ is $n \times m$, then $AB$ and $BA$ have the same nonzero eigenvalues.

If $m>n$, then $AB$ has $m-n$ extra zero eigenvalues compared to $BA$.

#### Submatrices Have Smaller Singular Values

If  $B$ keeps $M \le m$ rows and $N \le n$ columns of $A$, then $||B|| \le ||A||$, as $||A||=\sigma_1$ when maximizing $||Ax||/||x||$ and the norm of a submatrix cannot be larger than the norm of a whole matrix: $\sigma_1(B) \le \sigma_1(A)$.

#### The Polar Decomposition $A=QS$

$A=U\Sigma V^T=(UV^T)(V\Sigma V^T)=(Q)(S)$, where $Q$ is orthogonal and $S$ is symmetric positive semidefinite.

If $A$ is invertible, then $\Sigma$ and $S$ are also invertible. $S$ is the symmetric positive definite square root of $A^TA$, because $S^2=V\Sigma^2V^T=A^TA$. So the eigenvalues of $S$ are the singular values of $A$. The eigenvectors of $S$ are the singular vectors $v$ of $A$.

The polar decomposition separates the rotation ($Q$) from the stretching. The eigenvalues of $S$ give the stretching factors.



## 1.9 Principle components and the Best Low Rank Matrix

Principle Component Analysis uses the largest $\sigma$'s connected to the first $u$ and $v$ to understand the information in a matrix of data:
$$
A_k=\sigma_1u_1v_1^T+...+\sigma_ku_kv_k^T, \; rank(A_k)=k
$$

#### Eckart-Young

With $A_k=\sigma_1u_1v_1^T+...+\sigma_ku_kv_k^T, \; rank(A_k)=k$,

![image-20221003160753442](C:\Users\21156\AppData\Roaming\Typora\typora-user-images\image-20221003160753442.png)

#### Three Norms

**Spectral norm ($l^2$ norm)**: $||A||_2=\max \frac{||Ax||}{||x||}=\sigma_1$

**Frobenius norm**: $||A||_F=\sqrt{\sigma_1^2+\cdots+\sigma_r^2}$

**Nuclear norm**: $||A||_N=\sigma_1+\cdots+\sigma_r$

For $n \times n$ identity matrix: $||I||_2=1, ||I||_F=\sqrt n, ||I||_N=n$.

Replace $I$ by any orthogonal matrix $Q$ and all the norms stay the same as all $\sigma_i=1$.

**All three norms have $||Q_1AQ_2^T||=||A||$ for orthogonal $Q_1$ and $Q_2$**.

#### Eckart-Young Theorem: Best Approximation by $A_k$