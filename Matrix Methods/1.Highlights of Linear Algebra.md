# Part1: Highlights of Linear Algebra

Five basic problems:
$$
Ax=b, \; Ax=\lambda x, \; Av=\sigma u, \; \min \frac{||Ax||^2}{||x||^2}, \; \bf{Factor\;the\;matrix\;A}
$$

- Is the vector $b$ in the column space of $A$?
- eigenvector directions -> $Ax$ keeps the same direction as $x$.
- The SVD finds its simplest pieces $\sigma uv^T$. Data science meets linear algebra in the SVD.
- Least squares and principal components.

## 1.1 Multiplication $Ax$ Using Columns of $A$

Matrix-vector multiplication, column space and the rank.

### Matrix-vector Multiplication

$$
\begin{bmatrix}
2 & 3 \\
2 & 4 \\
3 & 7 \\
\end{bmatrix}
\begin{bmatrix}
x_1 \\ x_2
\end{bmatrix}
=x_1\begin{bmatrix}
2 \\ 2 \\ 3
\end{bmatrix}+
x_2\begin{bmatrix}
3 \\ 4 \\ 7
\end{bmatrix}=
combinations\;of\;columns\;\alpha_1\;and\;\alpha_2
$$

$Ax$ is a **linear combination** of the columns of $A$.

**The combinations of the columns** fill out the **column space** of $A$.

$b=(b_1, b_2, b_3)$ is in the column space of $A$ exactly when $Ax=b$ has a solution $(x_1, x_2)$. The solution $x$ shows how to express the right side $b$ as a combination $x_1a_1+x_2a_2$ of the columns. For some $b$ this is impossible —— they are not in the column space.

#### Some subspaces of $R^3$

- The **zero vector** $(0, 0, 0)$ by itself.

- A **line** of all vectors $x_1a_1$.
- A **plane** of all vectors $x_1a_1+x_2a_2$.
- The **whole $R^3$** with all vectors $x_1a_1+x_2a_2+x_3a_3$.

And the vectors $a_1, a_2, a_3$ should be **independent**.

### Independent Columns and the Rank of $A$

A **basis** for a subspace is a full set of independent vectors: **All vectors in the space are combinations of the basis vectors.**

**The rank of a matrix  is the dimension of its column space.**

**The number of independent columns equals the number of independent rows.**

## 1.2 Matrix-Matrix Multiplication $AB$

Another way to compute $AB=C$: columns of $A$ times rows of $B$.

![image-20220917155512549](C:\Users\21156\AppData\Roaming\Typora\typora-user-images\image-20220917155512549.png)

All columns of $uv^T$ are multiples of $u=[2,2,1]^T$. All rows are multiples of $v^T=[3,4,6]$.

**Row rank = Column rank.**

**$r$ independent columns $\Leftrightarrow$ $r$ independent rows**.

![image-20220917160752388](C:\Users\21156\AppData\Roaming\Typora\typora-user-images\image-20220917160752388.png)

**Outer product**: $AB$ is the sum of columns $a_k$ times rows $b^*_k$, which are **rank one matrices**.

A dominant theme in applied linear algebra is closely related to the outer product: **Factor $A$ into $CR$ and look at the pieces $c_kr_k^*$ of $A=CR$**.

#### Five important factorizations

- $A=LU$ comes from **elimination**. The matrix $L$ is lower triangular and $U$ is upper triangular.
- $A=QR$ comes from **orthogonalizing** the columns $a_1$ to $a_n$ as in "Gram-Schmidt". $Q$ has orthonormal columns ($Q^TQ=I$) and $R$ is upper triangular.
- $S=Q\Lambda Q^T$ comes from the **eigenvalues** of a symmetric matrix $S=S^T$. Eigenvalues on the diagonal of $\Lambda$. Orthonormal eigenvectors in the columns of $Q$.
- $A=X\Lambda X^{-1}$ is **diagonalization** when $A$ is $n$ by $n$ with $n$ independent eigenvectors.
- $A=U\Sigma V^T$ is the **Singular Value Decomposition** of any matrix $A$ (square or not).

## 1.3 The Four Fundamental Subspaces

The **column space** $C(A)$ contains all combinations of the columns of $A$.

The **row space** $C(A)$ contains all combinations of the columns of $A^T$.

The **nullspace** $N(A)$ contains all solutions $x$ to $Ax=0$.

The **left nullspace** $N(A^T)$ contains all solutions $y$ to $A^Ty=0$.

**Counting Law:** $r$ independent equations $Ax=0$ have $n-r$ independent solutions.

#### From a graph

![image-20220918103437314](C:\Users\21156\AppData\Roaming\Typora\typora-user-images\image-20220918103437314.png)

![image-20220918103453011](C:\Users\21156\AppData\Roaming\Typora\typora-user-images\image-20220918103453011.png)

The **nullspace $N(A)$**: set $b=0$. The special solution $x=(1, 1, 1, 1)$ is a basis for $N(A)$. The dimension of $N(A)$ is 1.

The **column space $C(A)$**: There must be $n-r=4-1=3$ independent columns.

The **row space $C(A^T)$**: The same as for columns.

The **left nullspace $N(A^T)$**: The dimension $m-r=5-3=2$.

![image-20220918105331440](C:\Users\21156\AppData\Roaming\Typora\typora-user-images\image-20220918105331440.png)

![image-20220918105511754](C:\Users\21156\AppData\Roaming\Typora\typora-user-images\image-20220918105511754.png)

#### Ranks of $AB$ and $A+B$

##### inequalities and equalities for the rank

- $r(AB) \le R(A), r(AB) \le r(B)$
- $r(A+B)\le r(A)+r(B)$
- $r(A^TA)=r(AA^T)=r(A)=r(A^T)$
- $r(A^{m\times r})=r(B^{r\times n})=r \Rightarrow r(AB)=r$

## 1.4 Elimination and $A=LU$

Elimination factored $A=LU$ into a lower triangular $L$ times an upper triangular $U$. $Ax=b$ split into $Lc=b$ and $Ux=c$. 

Row-Column: $a_{ij}:=(a_{ij}-\sum a_{ik}a_{kj})/a_{ii}$.

Pivots can't be zero. Good codes will choose the largest number to be the pivot.

Every invertible $n$ by $n$ matrix $A$ leads to $PA=LU,P=$ **permutation**.

#### An example of LU factorization

$$
\begin{bmatrix}
1
\end{bmatrix}
$$



## 1.5 Orthogonal Matrices and Subspaces

#### 1. Orthogonal vectors

$||x-y||^2=||x||^2+||y||^2-2||x||\;||y||\cos\theta$.

Orthogonal vectors have $\cos\theta=0$, and the last term disappears.

#### 2. Orthogonal basis

**Hadamard matrices**, if divided by the length of columns, are orthogonal bases in 2, 4, 8, 16, ... dimensions.

**Fact**: Every subspace of $R^n$ has an orthogonal basis. For two independent vector $a$ and $b$, if $c=b-\frac{a^Tb}{a^Ta}a$, then $a^Tc=a^Tb-a^Tb=0$. (Gram-Schmidt idea: a basis becomes an orthogonal basis).

#### 3. Orthogonal subspaces

The row space of $A$ is orthogonal to the nullspace of $A$.

The column space of $A$ is orthogonal to the nullspace of $A^T$.
$$
Ax=\begin{bmatrix}
row\;1 \\
\dots \\
row\;m
\end{bmatrix}
\begin{bmatrix}
x
\end{bmatrix}=\begin{bmatrix}
0 \\
\dots \\
0
\end{bmatrix}
\qquad
A^Ty=\begin{bmatrix}
(column\;1)^T \\
\dots \\
(column\;n)^T
\end{bmatrix}
\begin{bmatrix}
y
\end{bmatrix}=\begin{bmatrix}
0 \\
\dots \\
0
\end{bmatrix}
$$
![image-20220918171103853](C:\Users\21156\AppData\Roaming\Typora\typora-user-images\image-20220918171103853.png)

Singular vectors $Av_r=\sigma_r u_r$ finds orthonormal bases $v_1, \dots, v_r$ (left part in the picture) for the row space of $A$, and $u_1, \dots, u_r$ (right part of the picture) for the column space of $A$.

#### 4. Tall thin $Q$ with orthonormal columns: $Q^TQ=I$

All the matrices $P=QQ^T$ have $P^2=P$: $P^2=(QQ^T)(QQ^T)=Q(Q^TQ)Q^T=QQ^T=P$.

![image-20220918174326713](C:\Users\21156\AppData\Roaming\Typora\typora-user-images\image-20220918174326713.png)

![image-20220918174348713](C:\Users\21156\AppData\Roaming\Typora\typora-user-images\image-20220918174348713.png)

#### 5 Orthogonal matrices

For 2 by 2, they are **rotations** of the plane or **reflections**.
$$
Q_{rotate}=\begin{bmatrix}
\cos\theta & -\sin\theta \\
\sin\theta & \cos\theta
\end{bmatrix} \\
Q_{reflect}=\begin{bmatrix}
\cos\theta & \sin\theta \\
\sin\theta & -\cos\theta
\end{bmatrix}
$$
$Q_{rotate}$ is rotation through an angle $\theta$. $Q_{reflect}$ is reflection across the $\frac{\theta}{2}$ - line.

$Q_1Q_2$ is still orthogonal: $(Q_1Q_2)^T(Q_1Q_2)=Q_2^TQ_1^TQ_1Q_2=Q_2^TQ_2=I$.

rotation times rotation = rotation. reflection times reflection = rotation.

rotation times reflection = reflection.

#### Householder Reflections

$H_n=I-2uu^T=I-\frac{2}{n}ones(n,n)$.

$H^TH=H^2=(I-2uu^T)(I-2uu^T)=I-4uu^T+4uu^Tuu^T=I$.

$Hu=(I-2uu^T)u=u-2u=-u$.

$Hw=(I-2uu^T)w=w,(u^Tw=0)$ .

## 1.6 Eigenvalues and eigenvectors

$Ax=\lambda x, A^2x=\lambda^2x,...,A^{-1}x=\frac{1}{\lambda}x$.

The eigenvectors of $A$ don't change direction when you multiply when them by $A$.  The output $Ax$ is on the same line as the input vector $x$.

**Properties**

- Trace of $S$: $\sum \lambda=trace(S)$.
- Determinant: $\Pi \lambda=det(S)$.
- Real eigenvalues: Symmetric matrices always have real eigenvalues.
- Orthogonal eigenvectors: $\lambda_i \ne \lambda_j \Rightarrow x_i \cdot x_j=0$.